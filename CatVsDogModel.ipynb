{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatVsDog Hyper-parameter Experimentation\n",
    "Developed by:\n",
    "* Astrid Thalía Arteaga Romero A01420220\n",
    "* Jorge Alexander Giovannetti Pulido A01283034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -  Packages\n",
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- load_dataset loads our dataset images into conveniently formatted numpy arrays.\n",
    "- utils provides the functions implemented in the \"Building your Deep Neural Network: Step by Step\" assignment with a couple tweaks for our experiments.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load_dataset import load_data\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Architecture of our model (L-layer Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims = [12288, 20, 7, 5, 1], learning_rate = 0.0075, num_iterations = 3000, activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"], print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    activation_functions -- list containing the activation function for each layer.\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(layers_dims) == len(activation_functions)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = [] # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters, activation_functions)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches, activation_functions)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Checking our model\n",
    "As a way of checking if our model is correct, we implement grad-checking to verify that our backward propagation works correctly in conjunction with our forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_cost(X, parameters, Y, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes the cost.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        input data, shape: number of features x number of examples.\n",
    "    parameters : dict\n",
    "        parameters to use in forward prop.\n",
    "    Y : array\n",
    "        true \"label\", shape: 1 x number of examples.\n",
    "    hidden_layers_activation_fn : str\n",
    "        activation function to be used on hidden layers: \"tanh\", \"relu\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        cross-entropy cost.\n",
    "    \"\"\"\n",
    "    # Compute forward prop\n",
    "    AL, _ = L_model_forward(X, parameters, activation_functions)\n",
    "\n",
    "    # Compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_check(parameters, gradients, X, Y, activation_functions, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the generated parameters\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    activation_functions -- list containing the activation function for each layer.\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Roll out parameters and gradients dictionaries\n",
    "    parameters_values = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "\n",
    "    # Create vector of zeros to be used with epsilon\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    for i in range(len(parameters_values)):\n",
    "        # Compute cost of theta + epsilon\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        j_plus = forward_prop_cost(X, vector_to_dictionary(theta_plus, layers_dims), Y, activation_functions)\n",
    "\n",
    "        # Compute cost of theta - epsilon\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        j_minus = forward_prop_cost(X, vector_to_dictionary(theta_minus, layers_dims), Y, activation_functions)\n",
    "\n",
    "        # Compute numerical gradients\n",
    "        gradapprox[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "\n",
    "    # Compute the difference of numerical and analytical gradients\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 10e-7:\n",
    "        print (\"There is a mistake in back-propagation implementation. The difference is: {}\".format(difference))\n",
    "    else:\n",
    "        print (\"The implementation of back-propagation is fine!. The difference is: {}\".format(difference))\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implementation of back-propagation is fine!. The difference is: 1.863935876725876e-09\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"]\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "X = np.random.randint(255, size=(12288, 5))/255.\n",
    "Y = np.random.randint(0, 2, size=(1, 5))\n",
    "\n",
    "AL, caches = L_model_forward(X, parameters, activation_functions)\n",
    "cost = compute_cost(AL, Y)\n",
    "\n",
    "grads = L_model_backward(AL, Y, caches, activation_functions)\n",
    "difference = gradient_check(parameters, grads, X, Y, activation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Dataset\n",
    "The dataset we selected comes from Kaggle it is called \"Dogs & Cats Images\", there are 5000 pictures of cats and 5000 pictres of dogs.\n",
    "A 1000 cat and dog images are set for dev, another 1000 cat and dog images are set for testing and 3000 cat and dog images are set for training.\n",
    "\n",
    "LINK:https://www.kaggle.com/chetankv/dogs-cats-images\n",
    "\n",
    "We'll use our dev_set for experimenting and when we reach our objective we'll train the model with the train set and, finally, test our results with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 628.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 618.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:04<00:00, 651.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:04<00:00, 615.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 649.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 657.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_x_orig, dev_y, classes = load_data('dev_set')\n",
    "train_x_orig, train_y, classes = load_data('training_set')\n",
    "test_x_orig, test_y, classes = load_data('test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 6000\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (6000, 64, 64, 3)\n",
      "train_y shape: (1, 6000)\n"
     ]
    }
   ],
   "source": [
    "# Exploring our dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of train examples: \" + str(m_train))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_x's shape: (12288, 2000)\n",
      "train_x's shape: (12288, 6000)\n",
      "test_x's shape: (12288, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "dev_x_flatten = dev_x_orig.reshape(dev_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "dev_x = dev_x_flatten/255.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"dev_x's shape: \" + str(dev_x.shape))\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #REMOVE LATER\n",
    "# dev_x_orig = dev_x_orig[:10]\n",
    "# dev_y = dev_y[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12,288$ equals $64 \\times 64 \\times 3$ which is the size of one reshaped image vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Hyper-parameters\n",
    "\n",
    "We decided to experiment with the following hyper-parameters:\n",
    "- Learning rate\n",
    "- Number of iterations\n",
    "- Number of layers\n",
    "- Number of neurons in each layer\n",
    "- The activation function in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [train_x.shape[0], 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment #1: Default hyper-parameters\n",
    "As a way of getting a baseline result for our experimentation process, we decided to use the same hyper-parameters we used in class for our first experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 646.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 643.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_x's shape: (12288, 2000)\n"
     ]
    }
   ],
   "source": [
    "dev_x_orig, dev_y, classes = load_data('dev_set')\n",
    "dev_x_flatten = dev_x_orig.reshape(dev_x_orig.shape[0], -1).T\n",
    "dev_x = dev_x_flatten/255.\n",
    "print (\"dev_x's shape: \" + str(dev_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0075\n",
    "num_iterations = 2500\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"]\n",
    "\n",
    "parameters = L_layer_model(dev_x, dev_y, layers_dims, learning_rate, num_iterations, activation_functions, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing experiment #1 results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6 - Final Performance Estimation\n",
    "To evaluate the performance of our model as per the last experiment, we test the results with our test_set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters, activation_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters, activation_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-76d78ca5a144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_mislabeled_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Education\\ITESM\\6th Sem\\Deep Learning\\Final Project\\CatVsDogBinaryClassifier\\utils.py\u001b[0m in \u001b[0;36mprint_mislabeled_images\u001b[1;34m(classes, X, y, p, img_size)\u001b[0m\n\u001b[0;32m    399\u001b[0m                    interpolation='nearest')\n\u001b[0;32m    400\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         plt.title(\"Prediction: \" + classes[int(p[0, index])].decode(\n\u001b[0m\u001b[0;32m    402\u001b[0m             \"utf-8\") + \" \\n Class: \" + classes[y[0, index]].decode(\"utf-8\"))\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAANUlEQVR4nGP8//8/AyWAiSLdowbQwYCe7rn/l69Yhz+e////jw//nzVjyX98ahhHE9JwMAAArOc8bbwtl7gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 2880x2880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
