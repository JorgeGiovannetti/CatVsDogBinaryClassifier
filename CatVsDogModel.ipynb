{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatVsDog Hyper-parameter Experimentation\n",
    "Developed by:\n",
    "* Astrid Thalía Arteaga Romero A01420220\n",
    "* Jorge Alexander Giovannetti Pulido A01283034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -  Packages\n",
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- load_dataset loads our dataset images into conveniently formatted numpy arrays.\n",
    "- utils provides the functions implemented in the \"Building your Deep Neural Network: Step by Step\" assignment with a couple tweaks for our experiments.\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load_dataset import load_data\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Architecture of our model (L-layer Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims = [12288, 20, 7, 5, 1], learning_rate = 0.0075, num_iterations = 3000, activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"], print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    activation_functions -- list containing the activation function for each layer.\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(layers_dims) == len(activation_functions)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = [] # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters, activation_functions)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches, activation_functions)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Checking our model\n",
    "As a way of checking if our model is correct, we implement grad-checking to verify that our backward propagation works correctly in conjunction with our forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_cost(X, parameters, Y, activation_functions):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes the cost.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        input data, shape: number of features x number of examples.\n",
    "    parameters : dict\n",
    "        parameters to use in forward prop.\n",
    "    Y : array\n",
    "        true \"label\", shape: 1 x number of examples.\n",
    "    hidden_layers_activation_fn : str\n",
    "        activation function to be used on hidden layers: \"tanh\", \"relu\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        cross-entropy cost.\n",
    "    \"\"\"\n",
    "    # Compute forward prop\n",
    "    AL, _ = L_model_forward(X, parameters, activation_functions)\n",
    "\n",
    "    # Compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def gradient_check(parameters, gradients, X, Y, activation_functions, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the generated parameters\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    activation_functions -- list containing the activation function for each layer.\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Roll out parameters and gradients dictionaries\n",
    "    parameters_values = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "\n",
    "    # Create vector of zeros to be used with epsilon\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    for i in range(len(parameters_values)):\n",
    "        # Compute cost of theta + epsilon\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        j_plus = forward_prop_cost(X, vector_to_dictionary(theta_plus, layers_dims), Y, activation_functions)\n",
    "\n",
    "        # Compute cost of theta - epsilon\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        j_minus = forward_prop_cost(X, vector_to_dictionary(theta_minus, layers_dims), Y, activation_functions)\n",
    "\n",
    "        # Compute numerical gradients\n",
    "        gradapprox[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "\n",
    "    # Compute the difference of numerical and analytical gradients\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 10e-7:\n",
    "        print (\"\\033[31mThere is a mistake in back-propagation \" +\\\n",
    "               \"implementation. The difference is: {}\".format(difference))\n",
    "    else:\n",
    "        print (\"\\033[32mThere implementation of back-propagation is fine! \")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThere implementation of back-propagation is fine! The difference is: nan\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"]\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "X = np.random.randint(255, size=(12288, 5))\n",
    "Y = np.random.randint(0, 2, size=(1, 5))\n",
    "\n",
    "AL, caches = L_model_forward(X, parameters, activation_functions)\n",
    "cost = compute_cost(AL, Y)\n",
    "grads = L_model_backward(AL, Y, caches, activation_functions)\n",
    "difference = gradient_check(parameters, grads, X, Y, activation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Dataset\n",
    "The dataset we selected comes from Kaggle it is called \"Dogs & Cats Images\", there are 5000 pictures of cats and 5000 pictres of dogs.\n",
    "A 1000 cat and dog images are set for dev, another 1000 cat and dog images are set for testing and 3000 cat and dog images are set for training.\n",
    "\n",
    "LINK:https://www.kaggle.com/chetankv/dogs-cats-images\n",
    "\n",
    "We'll use our dev_set for experimenting and when we reach our objective we'll train the model with the train set and, finally, test our results with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 578.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 573.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:04<00:00, 603.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:05<00:00, 574.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 582.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 619.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_x_orig, dev_y, classes = load_data('dev_set')\n",
    "train_x_orig, train_y, classes = load_data('training_set')\n",
    "test_x_orig, test_y, classes = load_data('test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 6000\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (6000, 64, 64, 3)\n",
      "train_y shape: (1, 6000)\n"
     ]
    }
   ],
   "source": [
    "# Exploring our dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of train examples: \" + str(m_train))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_x's shape: (12288, 2000)\n",
      "train_x's shape: (12288, 6000)\n",
      "test_x's shape: (12288, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "dev_x_flatten = dev_x_orig.reshape(dev_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "dev_x = dev_x_flatten/255.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"dev_x's shape: \" + str(dev_x.shape))\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12,288$ equals $64 \\times 64 \\times 3$ which is the size of one reshaped image vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Hyper-parameters\n",
    "\n",
    "We decided to experiment with the following hyper-parameters:\n",
    "- Learning rate\n",
    "- Number of iterations\n",
    "- Number of layers\n",
    "- Number of neurons in each layer\n",
    "- The activation function in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment #1: Default hyper-parameters\n",
    "As a way of getting a baseline result for our experimentation process, we decided to use the same hyper-parameters we used in class for our first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.716532\n",
      "Cost after iteration 100: 0.693150\n",
      "Cost after iteration 200: 0.692789\n",
      "Cost after iteration 300: 0.683020\n",
      "Cost after iteration 400: 0.693306\n",
      "Cost after iteration 500: 0.693253\n",
      "Cost after iteration 600: 0.693253\n",
      "Cost after iteration 700: 0.693253\n",
      "Cost after iteration 800: 0.693253\n",
      "Cost after iteration 900: 0.693253\n",
      "Cost after iteration 1000: 0.693253\n",
      "Cost after iteration 1100: 0.693253\n",
      "Cost after iteration 1200: 0.693253\n",
      "Cost after iteration 1300: 0.693253\n",
      "Cost after iteration 1400: 0.693253\n",
      "Cost after iteration 1500: 0.693253\n",
      "Cost after iteration 1600: 0.693253\n",
      "Cost after iteration 1700: 0.693253\n",
      "Cost after iteration 1800: 0.693253\n",
      "Cost after iteration 1900: 0.693253\n",
      "Cost after iteration 2000: 0.693253\n",
      "Cost after iteration 2100: 0.693253\n",
      "Cost after iteration 2200: 0.693253\n",
      "Cost after iteration 2300: 0.693253\n",
      "Cost after iteration 2400: 0.693253\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.0075\n",
    "num_iterations = 2500\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"]\n",
    "\n",
    "parameters = L_layer_model(dev_x, dev_y, layers_dims, learning_rate, num_iterations, activation_functions, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing experiment #1 results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6 - Final Performance Estimation\n",
    "To evaluate the performance of our model as per the last experiment, we test the results with our test_set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters, activation_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters, activation_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
